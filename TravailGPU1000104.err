/var/spool/slurmd/job1000104/slurm_script: line 16: conda: command not found
Loading openmpi/4.1.5-cuda
  Loading requirement: intel-compilers/2021.9.0
Loading magma/2.7.1-cuda
  Unloading dependent: openmpi/4.1.5-cuda
  Loading requirement: nvidia-compilers/23.1
  Reloading dependent: openmpi/4.1.5-cuda
+ cd src
+ n_gpu=1
+ n_input=100
+ n_input_judge=500
+ :
+ MODEL_PATH=/lustre/fswork/projects/rech/mpz/uip95qy/Qwen2.5-0.5B
+ VLLM_PORT=8000
+ LOGFILE=vllm_server.log
+ SERVER_URL=http://localhost:8000/health
+ echo '[INFO] Starting vLLM server...'
+ VLLM_PID=1555314
+ vllm serve /lustre/fswork/projects/rech/mpz/uip95qy/Qwen2.5-0.5B
+ echo '[INFO] Waiting for vLLM server to become available...'
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
/gpfslocalsup/pub/anaconda-py3/2024.06/envs/vllm-0.5.4/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:46: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]

+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
+ curl --silent --fail http://localhost:8000/health
+ sleep 2
INFO:     Started server process [1555314]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
+ curl --silent --fail http://localhost:8000/health
+ echo '[INFO] vLLM server is up.'
+ python tree_search.py
+ python process_data.py tree 1
+ echo '[INFO] Shutting down vLLM server...'
+ kill 1555314
+ echo '[INFO] All tasks completed.'
